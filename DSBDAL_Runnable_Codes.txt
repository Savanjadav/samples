**Note:https://drive.google.com/drive/folders/1nr_RFa_QwjfNMCh22iOAzvDiZO7MJ3iu 
 Dataset for the programs☝ **
** Pycharm IDE and anaconda interpreter is used.
Note: Below given codes are only for learning and educational purposes. The Owner is not responsible for any malpractices. 
Data-Wrangling 1
import matplotlib
import pandas as pd
import matplotlib.pylab as plt
import numpy as np


df = pd.read_csv("dataset.csv")
print("\n",df.head(10))
print("\n",df.tail())
print("\n",df.info())
df.describe()
print("\n",df.isnull())
print("\n",df.isnull().sum())
print("\n",df.notnull())
print("\n",df.notnull().sum())


#caLculate the mean value for "stroke" column
avg_stroke = df["stroke"].astype("float").mean(axis =0)
print("\n Average of stroke :",avg_stroke)


#replace NaN by mean value in "stroke" column
df["stroke"].replace(np.nan, avg_stroke,inplace =True)


#Calculate the mean value for the 'horsepower' column :
avg_hp=df["horsepower"].astype("float").mean(axis = 0)
print("\n Average of stroke :",avg_hp)


df['horsepower'].replace(np.nan,avg_hp,inplace = True)
from contextlib import nullcontext
print("\n",df['num-of-doors'].value_counts())
print("\n",df['num-of-doors'].value_counts().idxmax())




# replace the missing 'num-of-door' values by most frequent
df['num-of-doors'].replace(np.nan, "four" , inplace=True)




#simply drop whole row with nan in "Horsepower-banned" column
df.dropna(subset=['horsepower-binned'], axis=0 , inplace=True)




#reset index, because we dropped two rows
df.reset_index(drop=True, inplace=True)
print(df.isnull().sum())


#DATA STANDARDIZATION : It is process of transforming data into common
# format which allowsthe researcher to make meaningful comparision


df['city-L/100km']=235/df['city-mpg']
print("\n",df.head())


df['highway-L/100km']=235/df["highway-mpg"]
print("\n",df['highway-L/100km'].head())


#DATA NORMALIZATION : It is process of transforming several values into similar range
df['length']=df['length']/df['length'].max()
df['width']=df['width']/df['width'].max()
df['height']=df['height']/df['height'].max()


print(df[['length','width','height']].head())


#INDICATOR VARIABLE : Indicator variable or dummy variable are used to
# label numerical variableused to label categories
print(df.columns)


print(df['aspiration'].value_counts())
dummy_var_1=pd.get_dummies(df['aspiration'])
print(dummy_var_1.head())
df=pd.concat([df,dummy_var_1], axis=1)
df.drop('aspiration',axis =1, inplace =True)
print("\n",df.head())
print(df.columns)


# BINNING:
# It is process of transforming continous data into discrete categorical 'bins' for groupanalysis


df ["horsepower"]=df ["horsepower"].astype(float, copy=True)


#%matplotlib inline
import matplotlib.pyplot as plt


plt.hist(df['wheel-base'])
plt.xlabel('wheel-base')
plt.ylabel('count')
plt.title('wheel-base bins')
plt.show()




bins = np.linspace(min(df['wheel-base']),max(df['wheel-base']),4)
print(bins)




















































































Data-Wrangling 2
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
df=pd.read_csv("StudentsPerformance.csv")


print(df.head(10))
print(df.shape)


print(df.dtypes.value_counts())


df.info()


print(df.describe())


# Handle the Missing Value
print(df.isnull().sum())


# Making list of columns having missing value
data = df
coln=[]
miss=[]
coln.extend(data.columns)
for i in coln:
   t=data[i].isnull
   if t!=0:
       miss.append(i)
   else:
       continue
print(miss)


pd.options.mode.chained_assignment=None
for j in miss:
   q=data[j].dtypes
   if(q=='int64'or q=='float64'):
       f=data[j]
       for k in range (data.shape[0]):
           if(f[k]<0 or f[k]>100):
               f[k]=(np.nan)
           else:
               data.fillna(method='bfill')


print(data.head(20))


data['math score'].plot(kind='box')
plt.show()


data['reading score'].plot(kind='box')
plt.show()


data['writing score'].plot(kind='box')
plt.show()


print(data.head(10))


# Outliers Removal
q1=data['math score'].quantile(0.25)
q3=data['math score'].quantile(0.75)
iqr=q3-q1
lowerlimit=q1-1.5*iqr
upperlimit =q3 + 1.5*iqr
print("Q1",q1, "\nQ3:" , q3,"\nIQR:",iqr, "\nLOWER LIMIT",lowerlimit,"\nUPPER LIMIT",upperlimit)


print(data[(data['math score']<lowerlimit)|(data['math score']>upperlimit)])
print(data[(data['math score']<lowerlimit)&(data['math score']>upperlimit)])


data['math score'].plot(kind='box')
plt.show()


# Z-Score
new_data=data
from scipy import stats
columns=['math score','reading score','writing score']
new_data[columns]= stats.zscore(new_data[columns])
print(new_data)


# Min-Max Scaling
new_data1=data
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
col=['math score','reading score','writing score']
scaler.fit(new_data1[col])
new_data1[col]=scaler.transform(new_data1[col])
print(new_data1)
















Data-Analytics-1


#Data Analytics I
# Create a Linear Regression Model using Python/R to predict home prices using Boston Housing
# Dataset (https://www.kaggle.com/c/boston-housing). The Boston Housing dataset contains
# information about various houses in Boston through different parameters. There are 506 samples
# and 14 feature variables in this dataset.




# The objective is to predict the value of prices of the house using the given features


import numpy as np
import pandas as pd
import matplotlib.pyplot as plt


from sklearn.datasets import load_boston
boston = load_boston()


boston.data.shape


boston.feature_names


data = pd.DataFrame(boston.data)
data.columns = boston.feature_names


data.head(15)


boston.target.shape


data['Price'] = boston.target
data.head()


data.describe()


data.info()


x=boston.data
y=boston.target


from sklearn.model_selection import train_test_split


xtrain,xtest,ytrain,ytest = train_test_split(x,y, test_size = 0.2)


print("xtrain shape :",xtrain.shape)
print("xtest shape :",xtest.shape)
print("ytrain shape :",ytrain.shape)
print("ytest shape :", ytest.shape)


from sklearn.linear_model import LinearRegression
regressor = LinearRegression()
regressor.fit(xtrain,ytrain)


y_pred = regressor.predict(xtest)


plt.scatter(ytest,y_pred, c = 'orange')
plt.xlabel("Price : in $1000's")
plt.ylabel("Predicted value")
plt.title("True value vs Predicted value : Linear Regression")
plt.show()


from sklearn.metrics import mean_squared_error
mse = mean_squared_error(ytest,y_pred)
print("Mean Square Error :",mse)


















































Data-Analytics-2
# Data Analytics II
# 1. Implement logistic regression using Python/R to perform classification on
# Social_Network_Ads.csv dataset.
# 2. Compute Confusion matrix to find TP, FP, TN, FN, Accuracy, Error rate, Precision, Recall
# on the given dataset




import numpy as np
import pandas as pd
import matplotlib.pyplot as mtp


df=pd.read_csv("Social_Network_Ads.csv")


df.head()


df.describe()


df.isnull().sum()


df.shape


df.info()


x = df.iloc[:,2:4]
y = df.iloc[:,4]


print(x)


from sklearn.model_selection import train_test_split


x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=42)


from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix,ConfusionMatrixDisplay,classification_report,accuracy_score, precision_score, recall_score, f1_score


scale = StandardScaler()
x_train = scale.fit_transform(x_train)
x_test = scale.transform(x_test)


lr = LogisticRegression(random_state=0, solver='lbfgs')
lr.fit(x_train, y_train)
pred = lr.predict(x_test)


x1=df.iloc[:, 0].values
y1=df.iloc[:, 4].values
mtp.scatter(x1,y1,color='purple',s=50)
mtp.xlabel('UserID')
mtp.ylabel('Purchased')
mtp.title('Userid VS purchased')
mtp.show()


x2=df.iloc[:, 1].values
y2=df.iloc[:, 4].values
mtp.scatter(x2,y2,color='green',s=50)
mtp.xlabel('Gender')
mtp.ylabel('Purchased')
mtp.title('Gender VS purchased')
mtp.show()


x3=df.iloc[:, 2].values
y3=df.iloc[:, 4].values
mtp.scatter(x3,y3,color='red',s=50)
mtp.xlabel('Age')
mtp.ylabel('Purchased')
mtp.title('Age VS purchased')
mtp.show()


x4=df.iloc[:, 3].values
y4=df.iloc[:, 4].values
mtp.scatter(x4,y4,color='orange',s=50)
mtp.xlabel('EstimatedSalary')
mtp.ylabel('Purchased')
mtp.title('EstimatedSalary VS purchased')
mtp.show()


import seaborn as sns
mtp.figure(figsize=(7,4))
sns.heatmap(df.corr(),annot=True,cmap='cubehelix_r')
mtp.show()


matrix = confusion_matrix(y_test, pred, labels= lr.classes_)


conf_matrix = ConfusionMatrixDisplay(confusion_matrix=matrix,display_labels=lr.classes_)


conf_matrix.plot(cmap=mtp.cm.Blues)
mtp.show()
Data-Analytics-3
# Data Analytics III
# 1. Implement Simple Naïve Bayes classification algorithm using Python/R on iris.csv dataset.
# 2. Compute Confusion matrix to find TP, FP, TN, FN, Accuracy, Error rate, Precision, Recall
# on the given dataset.
#


import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.image as mpimg


dataset= pd.read_csv('Iris.csv')


dataset.head()


# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
img=mpimg.imread('iris_types.jpg')
plt.figure(figsize=(10,30))
plt.axis('off')
plt.imshow(img)


x=dataset.iloc[:,:4].values
y=dataset['Variety'].values


from pandas.core.common import random_state
from sklearn.model_selection import train_test_split
X_train, X_test , y_train, y_test=train_test_split(x,y, test_size = 0.20)


from sklearn.preprocessing import StandardScaler
sc= StandardScaler()
X_train=sc.fit_transform(X_train)
X_test=sc.transform(X_test)


from sklearn.naive_bayes import GaussianNB
nvclassifier=GaussianNB()
nvclassifier.fit(X_train,y_train)


y_pred= nvclassifier.predict(X_test)
print(y_pred)


y_compare=np.vstack((y_test,y_pred)).T
y_compare[:5,:]


from sklearn.metrics import confusion_matrix
cm= confusion_matrix(y_test,y_pred)
print(cm)


a=cm.shape
corrPred=0
falsePred=0


for row in range(a[0]):
 for c in range(a[1]):
   if row == c :
     corrPred += cm[row,c]
   else :
     falsePred += cm[row,c]


print('correct Prediction',corrPred)
print('false Prediction',falsePred)
print('\n')
print('\n Accuracy of Naive Bayes Classification',corrPred/(cm.sum()))
print('\n ErrorRate of Naive Bayes Classification',falsePred/(cm.sum()))






















































Descriptive-Statistics
import pandas as pd
import numpy as np
# Load the data
# Download the Iris.csv file from https://www.kaggle.com/datasets/saurabh00007/iriscsv
# keep the .csv file in the same folder.
df = pd.read_csv("Iris.csv")
print(df.shape)
print(df.info())


# 50th Percentile
def q50(x):
  return x.quantile(0.5)


# 90th Percentile
def q90(x):
  return x.quantile(0.9)


## Solution For Problem Statement 1
print("\n Problem Solution 1: ")
print( "\nProviding summary statistics for Species(categorical variable) groupedBy Sepal-Length(quantitative variable)\n")
print(df.groupby(['Species'])[['SepalLengthCm']].agg(['mean','median','min','max','std',q50,q90]))
print( "\nProviding summary statistics for Species(categorical variable) groupedBy Sepal-Width(quantitative variable)")
print(df.groupby(['Species'])[['SepalWidthCm']].agg(['mean','median','min','max','std',q50,q90]))
print( "\nProviding summary statistics for Species(categorical variable) groupedBy Petal-Length(quantitative variable)")
print(df.groupby(['Species'])[['PetalLengthCm']].agg(['mean','median','min','max','std',q50,q90]))
print( "\nProviding summary statistics for Species(categorical variable) groupedBy Petal-Width(quantitative variable)")
print(df.groupby(['Species'])[['PetalWidthCm']].agg(['mean','median','min','max','std',q50,q90]))


## Solution For Problem Statement 2
print("\n Problem Solution 2: ")
print("\n Mean of dataset groupedby Species")
print(df.groupby(['Species'])[['SepalLengthCm','SepalWidthCm','PetalLengthCm','PetalWidthCm']].mean())


print("\n Median of dataset groupedby Species")
print(df.groupby(['Species'])[['SepalLengthCm','SepalWidthCm','PetalLengthCm','PetalWidthCm']].median())


print("\n50 Percentile result for 'Species' in Iris dataframe")
print(df.groupby(['Species'])[['SepalLengthCm','SepalWidthCm','PetalLengthCm','PetalWidthCm']].quantile(0.5))


print("\n90 Percentile result for 'Species' in Iris dataframe")
print(df.groupby(['Species'])[['SepalLengthCm','SepalWidthCm','PetalLengthCm','PetalWidthCm']].quantile(0.9))


print("\n Standard Deviation of dataset groupedby Species")
print(df.groupby(['Species'])[['SepalLengthCm','SepalWidthCm','PetalLengthCm','PetalWidthCm']].std())


































































Data-Visualization 1
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt


df = pd.read_csv("titanic-data.csv")
df.head()
# Finding different patterns in dataset
# Ticket class count for Pclasses
sns.set_theme(style="darkgrid")
ax = sns.countplot(x="Pclass", data=df)
ax = sns.countplot(x="Pclass", hue="Sex", data=df)
ax = sns.countplot(x="Embarked", data=df)




def plot(table, legloc='upper right',
        plt_style='seaborn-ticks',
        color_palette="dark", sorter=None, stacked=False,
        kind='bar', percentage=True,
        custom_title=None, minimal=True, figsize=(19, 10), width=0.7):
   grouped = table


   # Tranform to percentages
   if percentage == True:
       grouped = np.round(grouped.divide(grouped['Total'], axis=0) * 100, 0)
   try:
       del grouped['Total']
   except:
       pass


   # rearrange the columns
   if sorter:
       grouped = grouped[sorter]


   plt.style.use(plt_style)
   sns.set_palette(sns.color_palette(color_palette))
   ax = grouped.plot(kind=kind, stacked=stacked, figsize=figsize, width=width)
   _ = plt.setp(ax.get_xticklabels(), rotation=0)  # Rotate labels
   plt.legend(loc=legloc)  # plot the legend normally


   # annotate the bars
   if percentage == True:
       for p in ax.patches:
           ax.annotate('{}%'.format(int(np.round(p.get_height(), decimals=2))),
                       (p.get_x() + p.get_width() / 2.,
                        p.get_height()), ha='center', va='center',
                       xytext=(0, 10), textcoords='offset points')
   else:
       for p in ax.patches:
           ax.annotate(np.round(p.get_height(), decimals=2),
                       (p.get_x() + p.get_width() / 2.,
                        p.get_height()), ha='center', va='center',
                       xytext=(0, 10), textcoords='offset points')
   if minimal == True:
       ax.get_yaxis().set_ticks([])
       plt.xlabel('')
       sns.despine(top=True, right=True, left=True, bottom=False);
   else:
       pass
       # set custom title
   plt.title(custom_title)




def Groupby_TwoCol_Plot(df, col1, col2, legloc='upper right',
                       plt_style='ggplot',
                       color_palette="dark", sorter=None, stacked=False,
                       kind='bar', percentage=True,
                       custom_title=None, minimal=True, figsize=(14, 6), width=0.6):
   # Group by Placement and Representative and unstack by Placement
   grouped = df.groupby([col2, col1]).size().unstack(col2)


   # Make a totals column sort and delete after
   grouped['Total'] = grouped.sum(axis=1)
   # grouped = grouped.sort_values('Total', ascending = False)


   plot(grouped, legloc=legloc,
        plt_style=plt_style,
        color_palette=color_palette, sorter=sorter, stacked=stacked,
        kind=kind, percentage=percentage,
        custom_title=custom_title, minimal=minimal, figsize=figsize, width=width)




Groupby_TwoCol_Plot(df, 'Embarked', 'Survived', color_palette=('darkred', 'steelblue'),
                   plt_style='seaborn-ticks', custom_title='Proportion of Survived per Embarkation Port')
# Calculate percentages of port passengers per Class
Groupby_TwoCol_Plot(df, 'Embarked', 'Pclass', color_palette=('cubehelix'),
                   plt_style='seaborn-ticks', custom_title='Proportion of Embarked per PcClass', sorter=[1, 2, 3])
# Ticket Fare distribution among passengers


sns.displot(df['Fare'], kde=False, bins=10)
sns.barplot(x='Sex', y='Fare', data=df)
plt.figure(figsize=(8, 8))
sns.scatterplot(x="Age", y="Fare", hue="Sex", data=df)
plt.show()


















































































Data-Visualization 2
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt




df = pd.read_csv("titanic-data.csv")
df.head()
g = sns.FacetGrid(data=df,col="Sex",margin_titles=True)
g.map(sns.boxplot,"Survived","Age",order=[False,True])
plt.show()






















































































Data-Visualization 3
# **Data Visualization III**
#
# Download the Iris flower dataset or any other dataset into a DataFrame. (e.g.,
# https://archive.ics.uci.edu/ml/datasets/Iris ). Scan the dataset and give the inference as:
# 1. List down the features and their types (e.g., numeric, nominal) available in the dataset.
# 2. Create a histogram for each feature in the dataset to illustrate the feature distributions.
# 3. Create a boxplot for each feature in the dataset.
# 4. Compare distributions and identify outliers.


import pandas as pd
import numpy as np
df = pd.read_csv("iris.data")
df.head()
df.shape
df.describe()
df.tail()
# print(df[df.columns].mean())
# df.std()
# df.mode()
# df.cov()
# df.mode()
# df.median()
# df.var()


import seaborn as sns
import matplotlib.pyplot as plt
df.hist(bins=5)
df.hist()
plt.show()
df.columns
numeric_cols=['5.1','3.5','1.4','0.2','Iris-setosa']
np.min(df[numeric_cols])
np.max(df[numeric_cols])
df.quantile([0.0,0.1,0.5,1.0],numeric_only=True)
iris_long = pd.melt(df, id_vars='5.1')
ax = sns.boxplot(x="5.1", y="value",hue="variable", data=iris_long)


df['5.1'].value_counts()
df['5.1'].plot.density()
plt.show()
df.hist(bins=5)
sns.heatmap(df.corr(), annot=True)
plt.show()
#Correlation is feature to feature relation




































































Text Analytics
import nltk
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('averaged_perceptron_tagger')
nltk.download('wordnet')
sentence_data = "The First sentence is about Python. The Second: about Django. You can learn Python,Django and Data Ananlysis here."
nltk_tokens = nltk.sent_tokenize(sentence_data)
print(nltk_tokens)






nltk_tokens1 = nltk.word_tokenize(sentence_data)
print(nltk_tokens1)






import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize, sent_tokenize
stop_words = set(stopwords.words('english'))


#Dummy text
txt = "Sukanya, Rajib and Naba are my good friends. "\
"Sukanya is getting married next year. " \
"Marriage is a big step in one’s life." \
"It is both exciting and frightening. " \
"But friendship is a sacred bond between people." \
"It is a special kind of love between us. " \
"Many of you must have tried searching for a friend "\
"but never found the right one."


# sent_tokenize is one of instances of
# PunktSentenceTokenizer from the nltk.tokenize.punkt module


tokenized = sent_tokenize(txt)
for i in tokenized:


# Word tokenizers is used to find the words
# and punctuation in a string
   wordsList = nltk.word_tokenize(i)


# removing stop words from wordList
wordsList = [w for w in wordsList if not w in stop_words]


# Using a Tagger. Which is part-of-speech
# tagger or POS-tagger.
tagged = nltk.pos_tag(wordsList)


print(tagged)




from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize


example_sent = """This is a sample sentence,showing off the stop words filtration."""


stop_words = set(stopwords.words('english'))


word_tokens = word_tokenize(example_sent)


filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]


filtered_sentence = []


for w in word_tokens:
   if w not in stop_words:
       filtered_sentence.append(w)


print(word_tokens)
print(filtered_sentence)








# import these modules
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize


ps = PorterStemmer()


# choose some words to be stemmed
words = ["program", "programs", "programmer", "programming", "programmers"]


for w in words:
   print(w, " : ", ps.stem(w))




# import these modules
from nltk.stem import WordNetLemmatizer


lemmatizer = WordNetLemmatizer()


print("rocks :", lemmatizer.lemmatize("rocks"))
print("corpora :", lemmatizer.lemmatize("corpora"))


# a denotes adjective in "pos"
print("better :", lemmatizer.lemmatize("better", pos ="a"))




# import required module
from sklearn.feature_extraction.text import TfidfVectorizer


# assign documents
d0 = 'Geeks for geeks'
d1 = 'Geeks'
d2 = 'r2j'


# merge documents into a single corpus
string = [d0, d1, d2]


# create object
tfidf = TfidfVectorizer()


# get tf-df values
result = tfidf.fit_transform(string)


# get idf values
print('\nidf values:')
for ele1, ele2 in zip(tfidf.get_feature_names(), tfidf.idf_):
   print(ele1, ':', ele2)


# get indexing
print('\nWord indexes:')
print(tfidf.vocabulary_)


# display tf-idf values
print('\ntf-idf value:')
print(result)


# in matrix form
print('\ntf-idf values in matrix form:')
print(result.toarray())